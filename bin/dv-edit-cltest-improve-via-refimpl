#!/bin/bash
source $(dv-path lib/common.sh) # -*-mode: sh-mode -*-

dv-file-exists docs/user-request.md
dv-dir-exists cltest

basic="We need a highly detailed test suite for command-line use of this program.

Our format is  each test is in a file named like cltest/*.sh, to be run in bash. Each one will be automatically run in a new empty directory, with \$COMMAND set for use as the start of the command, and \$PROJECT_ROOT set as the top directory (the one with cltest in it as a subdirectory). The tests should not check or alter these environment variables.

Common common code or files can go in files named like cltest/_*

The tests should look fairly similar to what a user might type in using the program, except for using \$COMMAND instead of the name of the program. They should not clean up any files they create. Instead, they should leave them in place, in the working directory, to help with diagnosis.

The script should signal success by exit code 0, failure by exit code 1. Let the shell propagate these codes automatically, if that's shorter or simpler.
"

fix="$basic

Attached, please find the results of running the current draft of the cltest suite on the reference implementation. This shows one or more tests failing. By definition, any test failures on the reference implementation are an error in the test suite, so please fix the test suite.

You may also refactor, expand, or otherwise improve the test suite, if you see any way to make it better.
"

improve="$basic

Now is the time to carefully review the test suite and see if anything is missing or can otherwise be improved.

Here are some general ideas of what to test. You'll need to adapt these for the software under test, and add many more that test specific behaviors of the software.

## Basic Operations
- cltest/cli_launches_successfully.sh
- cltest/displays_help_with_h_flag.sh
- cltest/displays_version_with_v_flag.sh
- cltest/exits_with_zero_on_success.sh
- cltest/exits_with_nonzero_on_error.sh

## Input Handling
- cltest/accepts_valid_input_file.sh
- cltest/rejects_missing_input_file.sh
- cltest/handles_empty_input_file.sh
- cltest/validates_input_format.sh
- cltest/processes_unicode_input.sh

## Output Verification
- cltest/creates_expected_output_file.sh
- cltest/output_matches_expected_format.sh
- cltest/writes_logs_to_specified_location.sh
- cltest/respects_output_path_flag.sh
- cltest/overwrites_existing_output_when_forced.sh

## Option Parsing
- cltest/parses_short_options.sh
- cltest/parses_long_options.sh
- cltest/handles_combined_short_options.sh
- cltest/respects_option_precedence.sh
- cltest/validates_option_values.sh

## Error Conditions
- cltest/reports_invalid_options.sh
- cltest/handles_missing_required_args.sh
- cltest/provides_meaningful_error_messages.sh
- cltest/fails_gracefully_on_io_error.sh
- cltest/recovers_from_partial_failures.sh

## Interactive Features
- cltest/prompts_for_missing_required_input.sh
- cltest/handles_user_confirmation.sh
- cltest/supports_quiet_mode.sh
- cltest/respects_no_prompt_flag.sh
- cltest/allows_input_cancellation.sh
"

recent="$basic

We still have some test failures after several attempts. What's going on?  Let's think about this very carefully and see how we can address this.
"

# duplication with dv-edit-code-debug, but we might want to change them
# separately, so let's not factor it out.

passing=0
total=0

while true; do
    ((total++)) || true
    if [ $total -gt 15 ]; then
        log_error "Too many loops ($total)"
        exit 1
    }

    if dv-cltest; then
        ((passing++)) || true
        log_info passing $passing times
        if [ $passing -gt 1 ]; then
            echo "improved and passing!"
            exit 0
        fi
        export DV_PROMPT="$improve"
    else
        passing=0
        if [ "$total" -gt 9 ]; then
            dv-git-include 5
            export DV_PROMPT="$recent"
        else
            export DV_PROMPT="$fix"
        fi
    fi

    dv-edit
done
