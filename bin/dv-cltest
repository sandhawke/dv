#!/bin/bash
source $(dv-path lib/common.sh) # -*-mode: sh-mode -*-

start_dir="$(pwd)"
cd "$(dv-project-dir)"

# Some day we need to pick one, or be clear about what the difference is
export PROJECT_ROOT="$(pwd)"
export PACKAGE_ROOT="$PROJECT_ROOT"
DVD="$PWD/.dv"

summary_file="_to_developer/cltest-results-summary.txt"
log=$(dv-path-create "$DVD/cltest" run)
mkdir -p "$log"

echo "cltest running in $(pwd)"

if [ -z "${COMMAND:-}" ]; then
    [ -f .env ] && source .env
    if [ -z "${COMMAND:-}" ]; then
        log_error '$COMMAND needs to be set. See' "$summary_file"
        mkdir -p _to_developer
        echo 'Unable to run command-line tests because the environment variable $COMMAND is not set. It should generally be set by a line in the file ".env" like `export COMMAND="node $PROJECT_ROOT/src/cli.js"` or `export COMMAND="$PROJECT_ROOT/bin/foo"`. The exact value will depend on the language and layout of the package. Please set up that file and try again.' > $summary_file
        exit 1
    fi
fi

function say () {
    if [ -n "${DV_CLTEST_OUT:-}" ]; then
        echo "$@" >> "$DV_CLTEST_OUT"
    else
        echo "$@"
    fi
}

# Parse command line arguments
KEEP_RESULTS=0
declare -a TEST_FILES=()

# needs a mode where it output the files here

while [[ $# -gt 0 ]]; do
    case $1 in
        --keep)
            KEEP_RESULTS=1
            shift
            ;;
        *)
            TEST_FILES+=("$(realpath -m "$start_dir/$1")")
            shift
            ;;
    esac
done

# If no test files specified, use default glob pattern
if [ ${#TEST_FILES[@]} -eq 0 ]; then
    # Collect all matching test files from the default glob pattern
    while IFS= read -r -d '' file; do
        TEST_FILES+=("$file")
    done < <(find ./{cl,cli-,}test -maxdepth 1 -name '[a-z]*.sh' -type f -print0 2>/dev/null)
fi

# Reset output directory
rm -rf _to_developer/cltest-results
mkdir -p _to_developer/cltest-results

if dv-git-assert-clean --silent; then
    hash=$(git log -1 --format="%H")
    passing_dir="$(readlink -m .dv/cltest/by-commit/$hash/passing_tests)"
    if [ -d "$passing_dir" ]; then
        log_debug 'fyi: cltest already run on this repo version'
    fi
    rm -rf ".dv/cltest/latest/passing_tests"
else
    passing_dir="$(readlink -m .dv/cltest/latest/passing_tests)"
    # maybe dv-commit will save these away?
fi
mkdir -p "$passing_dir"
log_debug "passing_dir='$passing_dir'"

# Initialize counters
total_tests=0
passed_tests=0
failed_tests=0

# Array to store passed test directories for cleanup
declare -a passed_test_dirs

# Function to run a single test and record results
run_test() {
    local test_file="$1"
    local test_name=$(basename "$test_file" .sh)
    local result_dir="_to_developer/cltest-results/${test_name}"
    local working_dir="${result_dir}/working_directory"
    local tmp_dir="${result_dir}/tmp_directory"
    local absolute_test_path=$(readlink -f "$test_file")
    local value=''

    # Create test result directory and working directory
    mkdir -p "$result_dir"
    mkdir -p "$working_dir"
    mkdir -p "$tmp_dir"
    export WORKING_DIR="$working_dir"
    export TMP="$(readlink -f "$tmp_dir")"

    # echo running "$absolute_test_path"
    # echo stdout going to "$PWD/$result_dir/stdout"
    # Run the test in working directory and capture both stdout and stderr
    if (echo running; cd "$working_dir" && timeout 15s bash "$absolute_test_path") > "$result_dir/stdout" 2> "$result_dir/stderr"; then
        value=$?
        echo "PASS" > "$result_dir/status"
        touch "$log/pass.$test_name"
        touch "$passing_dir/$test_name"
        ((passed_tests++)) || true
        say -e "${GREEN}✓ ${test_name} passed${NC}"
        passed_test_dirs+=("$result_dir")
    else
        value=$?
        touch "$log/fail.$test_name"
        echo "FAIL, exit value $value" > "$result_dir/status"
        ((failed_tests++)) || true
        say -e "${MEDRED}✗ ${test_name} failed, see $result_dir${NC}"
    fi
    ((total_tests++)) || true
}

# Main execution
for test_file in "${TEST_FILES[@]}"; do
    if [ -f "$test_file" ]; then
        run_test "$test_file"
    else
        say -e "${MEDRED}Warning: Test file '$test_file' not found${NC}"
    fi
done

# Generate summary
if [ $total_tests -gt 0 ]; then
    let ppct=100*$passed_tests/$total_tests || true
    let fpct=100*$failed_tests/$total_tests || true
    say -e "\nTest Run Summary: Passed: ${GREEN}${passed_tests} (${ppct}%)${NC} Failed: ${MEDRED}${failed_tests} (${fpct}%)${NC}"
else
    say -e "${MEDRED}No tests were run${NC}"
    exit 1
fi

# Save summary to file
{
    echo "CL Test Run Summary"
    echo "==================="
    echo "Total tests: ${total_tests}"
    echo "Passed: ${passed_tests}"
    echo "Failed: ${failed_tests}"
    echo
    echo "Individual Test Results:"
    for test_file in "${TEST_FILES[@]}"; do
        if [ -f "$test_file" ]; then
            test_name=$(basename "$test_file" .sh)
            status=$(cat "_to_developer/cltest-results/${test_name}/status")
            echo "${test_name}: ${status}"
        fi
    done
} > "$summary_file"
cp -a "$summary_file" $log/summary.txt
(if cd $DVD/latest-edit 2> /dev/null; then
     echo $log >> cltest-runs.txt
fi)

# Clean up passed test directories unless --keep flag is provided
if [ $KEEP_RESULTS -eq 0 ]; then
    for dir in "${passed_test_dirs[@]}"; do
        rm -rf "$dir"
    done
fi

if [ $failed_tests -gt 0 ]; then
    log_warning "dv-cltest keeping details in $PWD/_to_developer/cltest-results"
    log_info "Saving stats in '$log'"
    exit 1
else
    log_success "All $passed_tests cltests passed!"
    exit 0
fi
