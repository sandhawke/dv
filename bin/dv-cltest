#!/bin/bash
source $(dv-path lib/common.sh) # -*-mode: sh-mode -*-

# Some day we need to pick one, or be clear about what the difference is
export PROJECT_ROOT=$PWD
export PACKAGE_ROOT=$PWD

summary_file="_to_developer/cltest-results-summary.txt"

if [ -z "${COMMAND:-}" ]; then
    [ -f .env ] && source .env
    if [ -z "${COMMAND:-}" ]; then
        log_error '$COMMAND needs to be set. See' "$summary_file"
        mkdir -p _to_developer
        echo 'Unable to run command-line tests because the environment variable $COMMAND is not set. It should generally be set by a line in the file ".env" like `export COMMAND="node $PROJECT_ROOT/src/cli.js"` or `export COMMAND="$PROJECT_ROOT/bin/foo"`. The exact value will depend on the language and layout of the package. Please set up that file and try again.' > $summary_file
        exit 1
    fi
fi

# Parse command line arguments
KEEP_RESULTS=0
for arg in "$@"; do
    case $arg in
        --keep)
            KEEP_RESULTS=1
            shift # Remove --keep from processing
            ;;
    esac
done

# Create output directory if it doesn't exist
mkdir -p _to_developer/cltest-results

# Initialize counters
total_tests=0
passed_tests=0
failed_tests=0

# Array to store passed test directories for cleanup
declare -a passed_test_dirs

# Function to run a single test and record results
run_test() {
    local test_file="$1"
    local test_name=$(basename "$test_file" .sh)
    local result_dir="_to_developer/cltest-results/${test_name}"
    local working_dir="${result_dir}/working_directory"
    local absolute_test_path=$(readlink -f "$test_file")
    local value=''

    # Create test result directory and working directory
    mkdir -p "$result_dir"
    mkdir -p "$working_dir"
    export WORKING_DIR="$working_dir"

    echo "Running test: ${test_name}"

    # Run the test in working directory and capture both stdout and stderr
    if (cd "$working_dir" && timeout 15s bash "$absolute_test_path") > "$result_dir/stdout" 2> "$result_dir/stderr"; then
        value=$?
        echo exit value PASS $value
        echo "PASS" > "$result_dir/status"
        ((passed_tests++)) || true # workaround
        echo "✓ ${test_name} passed"
        passed_test_dirs+=("$result_dir")

        # If DV_POINTS is set, calculate and record points for passed test
        if [ -n "${DV_POINTS:-}" ] && [ -n "${DV_POINTS_FILE:-}" ]; then
            # Count total number of test files
            local total_test_count=$(find {cl,cli-,}test -maxdepth 1 -name '[0-9a-zA-Z]*.sh' -type f | wc -l)
            # Calculate points per test
            local points_per_test=$(awk "BEGIN {printf \"%.2f\", ${DV_POINTS}/${total_test_count}}")
            # Append points and test name to points file
            echo "${points_per_test} ${test_name}" >> "${DV_POINTS_FILE}"
        fi
    else
        value=$?
        echo exit value FAIL $value
        echo "FAIL, exit value $value" > "$result_dir/status"
        ((failed_tests++)) || true # workaround bash being annoying
        echo "✗ ${test_name} failed, see $result_dir"
    fi
    ((total_tests++)) || true # workaround bash being annoying
}

# Main execution
echo "Starting test run..."
echo "==================="

# Find and run all matching test files
for test_file in {cl,cli-,}test/[0-9a-zA-Z]*.sh; do
    if [ -f "$test_file" ]; then
        run_test "$test_file"
    fi
done

# Generate summary
echo -e "\nTest Run Summary"
echo "================"
echo "Total tests: ${total_tests}"
echo "Passed: ${passed_tests}"
echo "Failed: ${failed_tests}"

# Save summary to file
{
    echo "CL Test Run Summary"
    echo "==================="
    echo "Total tests: ${total_tests}"
    echo "Passed: ${passed_tests}"
    echo "Failed: ${failed_tests}"
    echo
    echo "Individual Test Results:"
    for test_file in test/[0-9a-zA-Z]*.sh; do
        if [ -f "$test_file" ]; then
            test_name=$(basename "$test_file" .sh)
            status=$(cat "_to_developer/cltest-results/${test_name}/status")
            echo "${test_name}: ${status}"
        fi
    done
} > "$summary_file"
mkdir -p .dv/cltest-summaries
cp -a "$summary_file" .dv/cltest-summaries/$(uuidgen -r | cut -c 29-)
# maybe into edits/latest ?

# Clean up passed test directories unless --keep flag is provided
if [ $KEEP_RESULTS -eq 0 ]; then
    echo -e "\nCleaning up passed test directories..."
    for dir in "${passed_test_dirs[@]}"; do
        echo "Removing $dir"
        rm -rf "$dir"
    done
fi

if [ $failed_tests -gt 0 ]; then
    log_warning "dv-test exiting with $failed_tests failed tests, leaving files"
    find _to_developer/cltest-results -type f || log_error error in find
    exit 1
else
    log_success "All $passed_tests cltests passed!"
    exit 0
fi
