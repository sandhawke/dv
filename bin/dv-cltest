#!/bin/bash
source $(dv-path lib/common.sh) # -*-mode: sh-mode -*-

cd "$(dv-project-dir)"

# Some day we need to pick one, or be clear about what the difference is
export PROJECT_ROOT="$(pwd)"
export PACKAGE_ROOT="$PROJECT_ROOT"
DVD="$PWD/.dv"

summary_file="_to_developer/cltest-results-summary.txt"
log=$(dv-path-create "$DVD/cltest" run)
mkdir -p "$log"

echo "cltest running in $(pwd)"

if [ -z "${COMMAND:-}" ]; then
    [ -f .env ] && source .env
    if [ -z "${COMMAND:-}" ]; then
        log_error '$COMMAND needs to be set. See' "$summary_file"
        mkdir -p _to_developer
        echo 'Unable to run command-line tests because the environment variable $COMMAND is not set. It should generally be set by a line in the file ".env" like `export COMMAND="node $PROJECT_ROOT/src/cli.js"` or `export COMMAND="$PROJECT_ROOT/bin/foo"`. The exact value will depend on the language and layout of the package. Please set up that file and try again.' > $summary_file
        exit 1
    fi
fi

# Parse command line arguments
KEEP_RESULTS=0
declare -a TEST_FILES

# Process arguments
while [[ $# -gt 0 ]]; do
    case $1 in
        --keep)
            KEEP_RESULTS=1
            shift
            ;;
        *)
            TEST_FILES+=("$1")
            shift
            ;;
    esac
done

# If no test files specified, use default glob pattern
if [ ${#TEST_FILES[@]} -eq 0 ]; then
    # Collect all matching test files from the default glob pattern
    while IFS= read -r -d '' file; do
        TEST_FILES+=("$file")
    done < <(find ./{cl,cli-,}test -maxdepth 1 -name '[a-z]*.sh' -type f -print0 2>/dev/null)
fi

# Create output directory if it doesn't exist
mkdir -p _to_developer/cltest-results

# Initialize counters
total_tests=0
passed_tests=0
failed_tests=0

# Array to store passed test directories for cleanup
declare -a passed_test_dirs

# Function to run a single test and record results
run_test() {
    local test_file="$1"
    local test_name=$(basename "$test_file" .sh)
    local result_dir="_to_developer/cltest-results/${test_name}"
    local working_dir="${result_dir}/working_directory"
    local absolute_test_path=$(readlink -f "$test_file")
    local value=''

    # Create test result directory and working directory
    mkdir -p "$result_dir"
    mkdir -p "$working_dir"
    export WORKING_DIR="$working_dir"

    # Run the test in working directory and capture both stdout and stderr
    if (cd "$working_dir" && timeout 15s bash "$absolute_test_path") > "$result_dir/stdout" 2> "$result_dir/stderr"; then
        value=$?
        echo "PASS" > "$result_dir/status"
        touch "$log/pass.$test_name"
        ((passed_tests++)) || true
        echo -e "${GREEN}✓ ${test_name} passed${NC}"
        passed_test_dirs+=("$result_dir")

        # If DV_POINTS is set, calculate and record points for passed test
        if [ -n "${DV_POINTS:-}" ] && [ -n "${DV_POINTS_FILE:-}" ]; then
            # Count total number of test files
            local total_test_count=${#TEST_FILES[@]}
            # Calculate points per test
            local points_per_test=$(awk "BEGIN {printf \"%.2f\", ${DV_POINTS}/${total_test_count}}")
            # Append points and test name to points file
            echo "${points_per_test} ${test_name}" >> "${DV_POINTS_FILE}"
        fi
    else
        value=$?
        touch "$log/fail.$test_name"
        echo "FAIL, exit value $value" > "$result_dir/status"
        ((failed_tests++)) || true
        echo -e "${MEDRED}✗ ${test_name} failed, see $result_dir${NC}"
    fi
    ((total_tests++)) || true
}

# Main execution
for test_file in "${TEST_FILES[@]}"; do
    if [ -f "$test_file" ]; then
        run_test "$test_file"
    else
        echo -e "${MEDRED}Warning: Test file '$test_file' not found${NC}"
    fi
done

# Generate summary
if [ $total_tests -gt 0 ]; then
    let ppct=100*$passed_tests/$total_tests
    let fpct=100*$failed_tests/$total_tests
    echo -e "\nTest Run Summary: Passed: ${GREEN}${passed_tests} (${ppct}%)${NC} Failed: ${MEDRED}${failed_tests} (${fpct}%)${NC}"
else
    echo -e "${MEDRED}No tests were run${NC}"
    exit 1
fi

# Save summary to file
{
    echo "CL Test Run Summary"
    echo "==================="
    echo "Total tests: ${total_tests}"
    echo "Passed: ${passed_tests}"
    echo "Failed: ${failed_tests}"
    echo
    echo "Individual Test Results:"
    for test_file in "${TEST_FILES[@]}"; do
        if [ -f "$test_file" ]; then
            test_name=$(basename "$test_file" .sh)
            status=$(cat "_to_developer/cltest-results/${test_name}/status")
            echo "${test_name}: ${status}"
        fi
    done
} > "$summary_file"
cp -a "$summary_file" $log/summary.txt
(if cd $DVD/latest-edit 2> /dev/null; then
     echo $log >> cltest-runs.txt
fi)

# Clean up passed test directories unless --keep flag is provided
if [ $KEEP_RESULTS -eq 0 ]; then
    for dir in "${passed_test_dirs[@]}"; do
        rm -rf "$dir"
    done
fi

if [ $failed_tests -gt 0 ]; then
    log_warning "dv-cltest keeping details in _to_developer/cltest-results"
    log_info "Saving stats in '$log'"
    exit 1
else
    log_success "All $passed_tests cltests passed!"
    exit 0
fi
